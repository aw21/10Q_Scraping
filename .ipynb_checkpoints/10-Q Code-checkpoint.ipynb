{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing built-in libraries (no need to install these)\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta\n",
    "import unicodedata\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import shutil\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime, timedelta\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "from bs4 import NavigableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_directory = \"/Users/andrewwang/MyDocuments/10Q_Scraping\"\n",
    "os.chdir(original_directory)\n",
    "\n",
    "with open(\"SP500_Tickers.csv\") as f:\n",
    "    tickers = [row.split()[0] for row in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MapTickerToCik(tickers):\n",
    "    url = 'http://www.sec.gov/cgi-bin/browse-edgar?CIK={}&Find=Search&owner=exclude&action=getcompany'\n",
    "    cik_re = re.compile(r'.*CIK=(\\d{10}).*')\n",
    "\n",
    "    cik_dict = {}\n",
    "    for ticker in tqdm(tickers): # Use tqdm lib for progress bar\n",
    "        results = cik_re.findall(requests.get(url.format(ticker)).text)\n",
    "        if len(results):\n",
    "            cik_dict[str(ticker).lower()] = str(results[0])\n",
    "    \n",
    "    return cik_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 505/505 [02:36<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "cik_dict = MapTickerToCik(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mmm</th>\n",
       "      <td>0000066740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abt</th>\n",
       "      <td>0000001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbv</th>\n",
       "      <td>0001551152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abmd</th>\n",
       "      <td>0000815094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acn</th>\n",
       "      <td>0001467373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cik\n",
       "ticker            \n",
       "mmm     0000066740\n",
       "abt     0000001800\n",
       "abbv    0001551152\n",
       "abmd    0000815094\n",
       "acn     0001467373"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up the ticker-CIK mapping as a DataFrame\n",
    "ticker_cik_df = pd.DataFrame.from_dict(data=cik_dict, orient='index')\n",
    "ticker_cik_df.reset_index(inplace=True)\n",
    "ticker_cik_df.columns = ['ticker', 'cik']\n",
    "ticker_cik_df['cik'] = [str(cik) for cik in ticker_cik_df['cik']]\n",
    "ticker_cik_df = ticker_cik_df.set_index('ticker')\n",
    "ticker_cik_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WriteLogFile(log_file_name, text):\n",
    "    \n",
    "    '''\n",
    "    Helper function.\n",
    "    Writes a log file with all notes and\n",
    "    error messages from a scraping \"session\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "    text : str\n",
    "        Text to write to the log file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    with open(log_file_name, \"a\") as log_file:\n",
    "        log_file.write(text)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ScrapeDocument(ticker, browse_url_base, filing_url_base, doc_url_base, cik, log_file_name, is10K, num_files_to_scrape):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Ks and 10-K405s for a particular \n",
    "    CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(ticker)\n",
    "    except OSError:\n",
    "        text = f\"Already made folder for ticker {ticker}\"\n",
    "        WriteLogFile(log_file_name, text)\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    os.chdir(ticker)\n",
    "        \n",
    "    # Request list of 10-K filings\n",
    "    res = requests.get(browse_url_base.format(cik))\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.chdir('..')\n",
    "        os.rmdir(cik) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base.format(cik)) + '\\n'\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "\n",
    "    # If the request doesn't fail, continue...\n",
    "    \n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables)<3:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-K and 10-K405 document filings\n",
    "    if is10K:\n",
    "        filings_table = filings_table[(filings_table['Filings'] == '10-K') | (filings_table['Filings'] == '10-K405')]\n",
    "    else:\n",
    "        filings_table = filings_table[(filings_table['Filings'] == '10-Q')]\n",
    "        \n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    num_files_scraped = 0\n",
    "    \n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base.format(cik, acc_no))\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing\n",
    "        if docs_page.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base.format(cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "\n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        \n",
    "        filing_date_div = docs_page_soup.find(text=re.compile(\"Filing (D|d)ate\")).parent\n",
    "        filing_date = filing_date_div.findNext('div').get_text()\n",
    "        period_of_report_div = docs_page_soup.find(text=re.compile(\"Period (O|o)f (R|r)eport\")).parent\n",
    "        period_of_report_date = period_of_report_div.findNext('div').get_text()\n",
    "        \n",
    "        if is10K:\n",
    "            ticker_cik_df.at[ticker, f'10-K #{num_files_scraped + 1} Filing Date'] = filing_date\n",
    "            ticker_cik_df.at[ticker, f'10-K #{num_files_scraped + 1} Period'] = period_of_report_date\n",
    "        else:\n",
    "            ticker_cik_df.at[ticker, f'10-Q #{num_files_scraped + 1} Filing Date'] = filing_date\n",
    "            ticker_cik_df.at[ticker, f'10-Q #{num_files_scraped + 1} Period'] = period_of_report_date\n",
    "        \n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        if is10K:\n",
    "            docs_table = docs_table[(docs_table['Type'] == '10-K') | (docs_table['Type'] == '10-K405')]\n",
    "        else:\n",
    "            docs_table = docs_table[(docs_table['Type'] == '10-Q')]\n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            os.chdir('..')\n",
    "            text = 'File with CIK: {} and Acc_No: {} is unavailable'.format(cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        docname = docname.split()[0]\n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base.format(cik, acc_no.replace('-', ''), docname))\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base.format(cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "        \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'w')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'w')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "           \n",
    "        num_files_scraped = num_files_scraped + 1\n",
    "        \n",
    "        if num_files_scraped == num_files_to_scrape:\n",
    "            break\n",
    "        \n",
    "    # Move back to the main 10-K directory\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathname_10k = original_directory + '/10_K_Docs'\n",
    "pathname_10q = original_directory + '/10_Q_Docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_contents(foldername):\n",
    "    for root, dirs, files in os.walk(foldername):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "        \n",
    "\n",
    "\n",
    "delete_contents(pathname_10k)\n",
    "delete_contents(pathname_10q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [04:48,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "os.chdir(original_directory)\n",
    "# Run the function to scrape 10-K\n",
    "# Define parameters\n",
    "browse_url_base_10k = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-K'\n",
    "filing_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/{}/{}-index.html'\n",
    "doc_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/{}/{}/{}'\n",
    "\n",
    "# Set correct directory\n",
    "os.chdir(pathname_10k)\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "time = strftime(\"%Y-%m-%d %Hh%Mm%Ss\", gmtime())\n",
    "log_file_name = 'log '+time+'.txt'\n",
    "with open(log_file_name, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for ticker,row in tqdm(ticker_cik_df.iterrows()):\n",
    "    ScrapeDocument(ticker=ticker,\n",
    "                   browse_url_base=browse_url_base_10k, \n",
    "                   filing_url_base=filing_url_base_10k, \n",
    "                   doc_url_base=doc_url_base_10k, \n",
    "                   cik=row['cik'],\n",
    "                   log_file_name=log_file_name,\n",
    "                   is10K = True,\n",
    "                   num_files_to_scrape = 1)\n",
    "os.chdir(original_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "503it [26:55,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(original_directory)\n",
    "# Run the function to scrape 10-Qs\n",
    "# Define parameters\n",
    "browse_url_base_10q = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-Q&count=1000'\n",
    "filing_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/{}/{}-index.html'\n",
    "doc_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/{}/{}/{}'\n",
    "\n",
    "# Set correct directory\n",
    "os.chdir(pathname_10q)\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "time = strftime(\"%Y-%m-%d %Hh%Mm%Ss\", gmtime())\n",
    "log_file_name = 'log '+time+'.txt'\n",
    "with open(log_file_name, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for ticker,row in tqdm(ticker_cik_df.iterrows()):\n",
    "    ScrapeDocument(ticker=ticker,\n",
    "                   browse_url_base=browse_url_base_10q, \n",
    "                   filing_url_base=filing_url_base_10q, \n",
    "                   doc_url_base=doc_url_base_10q, \n",
    "                   cik=row['cik'],\n",
    "                   log_file_name=log_file_name,\n",
    "                   is10K = False,\n",
    "                   num_files_to_scrape = 3)\n",
    "    \n",
    "os.chdir(original_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ticker_cik_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fde69294d0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mticker_cik_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ticker_cik_df' is not defined"
     ]
    }
   ],
   "source": [
    "ticker_cik_df.head()\n",
    "ticker_cik_df.to_csv('ticker_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scrape text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def between(cur, end):\n",
    "    while cur and cur != end:\n",
    "        if isinstance(cur, NavigableString):\n",
    "            text = cur.strip()\n",
    "            if len(text):\n",
    "                yield text\n",
    "        cur = cur.next_element\n",
    "\n",
    "def get_risk_factor_text(ticker, is10K):\n",
    "    os.chdir(original_directory)\n",
    "    if is10K:\n",
    "        os.chdir(pathname_10k)\n",
    "    else:\n",
    "        os.chdir(pathname_10q)\n",
    "    cik = cik_dict[ticker]\n",
    "    os.chdir(cik)\n",
    "    file_name = os.listdir(\".\")[0]\n",
    "\n",
    "    with open(file_name) as file:\n",
    "        soup = bs.BeautifulSoup(file, \"html.parser\")\n",
    "    spans = soup.find_all('span')\n",
    "\n",
    "    risk_factor_span = None\n",
    "    after_risk_factor_span = None\n",
    "    \n",
    "    for span in spans:\n",
    "        text = span.get_text()\n",
    "        pattern1A = re.compile(\"item 1a(.*)\")\n",
    "        if pattern1A.match(text.lower()):\n",
    "            risk_factor_span = span\n",
    "        pattern2 = re.compile(\"item 2(.*)\")\n",
    "        if pattern2.match(text.lower()):\n",
    "            after_risk_factor_span = span\n",
    "            \n",
    "    if not risk_factor_span:\n",
    "        return []\n",
    "            \n",
    "    risk_factor_texts = [text.lower() for text in between(risk_factor_span, after_risk_factor_span)]\n",
    "\n",
    "    if len(risk_factor_texts) >= 2:\n",
    "        risk_factor_texts = risk_factor_texts[1:-1]\n",
    "    \n",
    "    full_text = ' '.join(risk_factor_texts)\n",
    "    sentences = split_into_sentences(full_text)\n",
    "    os.chdir(original_directory)\n",
    "    return sentences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(original_directory)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "negative_score = -4.0\n",
    "positive_score = 0.0\n",
    "uncertain_score = -2.0\n",
    "\n",
    "negative_words = pd.read_csv('LoughranMcDonald_SentimentWordLists_Negative.csv', header=None).iloc[:,0]\n",
    "positive_words = pd.read_csv('LoughranMcDonald_SentimentWordLists_Positive.csv', header=None).iloc[:,0]\n",
    "uncertain_words = pd.read_csv('LoughranMcDonald_SentimentWordLists_Uncertain.csv', header=None).iloc[:,0]\n",
    "\n",
    "negative_word_scores = {word.lower(): negative_score for word in negative_words}\n",
    "positive_word_scores = {word.lower(): positive_score for word in positive_words}\n",
    "uncertain_word_scores = {word.lower(): uncertain_score for word in uncertain_words}\n",
    "\n",
    "financial_word_dict = {**negative_word_scores, **positive_word_scores, **uncertain_word_scores}\n",
    "analyzer.lexicon.update(financial_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4563825925925926\n",
      "-0.5405285714285714\n",
      "-0.4442688888888889\n",
      "-0.4400178571428571\n",
      "-0.48643799126637555\n",
      "-0.7649075\n"
     ]
    }
   ],
   "source": [
    "# AAPL\n",
    "aapl_10k = get_risk_factor_text('aapl', is10K = True)\n",
    "aapl_text = get_risk_factor_text('aapl', is10K = False)\n",
    "dal_10k = get_risk_factor_text('dal', is10K = True)\n",
    "dal_text = get_risk_factor_text('dal', is10K = False)\n",
    "goog_10k = get_risk_factor_text('hlt', is10K = True)\n",
    "goog_text = get_risk_factor_text('hlt', is10K = False)\n",
    "\n",
    "aapl_scores_10k = {sentence: analyzer.polarity_scores(sentence)['compound'] for sentence in aapl_10k}\n",
    "aapl_scores = {sentence: analyzer.polarity_scores(sentence)['compound'] for sentence in aapl_text}\n",
    "dal_scores_10k = {sentence: analyzer.polarity_scores(sentence)['compound'] for sentence in dal_10k}\n",
    "dal_scores = {sentence: analyzer.polarity_scores(sentence)['compound'] for sentence in dal_text}\n",
    "goog_scores_10k = {sentence: analyzer.polarity_scores(sentence)['compound'] for sentence in goog_10k}\n",
    "goog_scores = {sentence: analyzer.polarity_scores(sentence)['compound'] for sentence in goog_text}\n",
    "\n",
    "print(np.mean(np.asarray(list(aapl_scores_10k.values()))))\n",
    "print(np.mean(np.asarray(list(aapl_scores.values()))))\n",
    "print(np.mean(np.asarray(list(dal_scores_10k.values()))))\n",
    "print(np.mean(np.asarray(list(dal_scores.values()))))\n",
    "print(np.mean(np.asarray(list(goog_scores_10k.values()))))\n",
    "print(np.mean(np.asarray(list(goog_scores.values()))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7348113821138211"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.asarray([val for val in list(goog_scores.values()) if val < 0 ]))\n",
    "np.mean(np.asarray([val for val in list(goog_scores_10k.values()) if val < 0 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.3851, 'neg': 0.104, 'neu': 0.715, 'pos': 0.181}"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores('There is no assuring that our efforts to obtain such an amendment or waiver would be successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.lexicon['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cik                 0000320193\n",
       "10-K Filing Date    2020-02-13\n",
       "10-K Period         2019-12-31\n",
       "10-Q Filing Date    2020-05-06\n",
       "10-Q Period         2020-03-31\n",
       "Name: aapl, dtype: object"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_cik_df.loc['aapl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2 >> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
